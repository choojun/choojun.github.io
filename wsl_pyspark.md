# G. PySpark
 
> 1. Spark is an in-memory distributed computing engine especially designed for the Hadoop cluster. 
> 2. Read more on Apache Spark at URL https://en.wikipedia.org/wiki/Apache_Spark

## G1. Installation
1.	Download the correct version of Spark, which is compatible with the installed Hadoop from the Apache Spark downloads page (http://spark.apache.org/downloads.html).
~~~
$ wget https://downloads.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3-scala2.13.tgz
~~~
